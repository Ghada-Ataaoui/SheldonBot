{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[{"file_id":"15wa925dj7jvdvrz8_z3vU7btqAFQLVlG","timestamp":1731350003768}],"gpuType":"T4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"e6fb4eb77cea40dd8ddfc989308cc62a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_622963dcc3e14f3cbc79c77914f6551f","IPY_MODEL_e74fe1bf0db04ea09d6c8dbaa1a50f43","IPY_MODEL_a89d1b4320bf471b9eaf573c37bbd2ba"],"layout":"IPY_MODEL_1b3c191b4c534509a4c6896ef30e3c84"}},"622963dcc3e14f3cbc79c77914f6551f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b2615b7f9d548c0b5c2e92af44d02e1","placeholder":"​","style":"IPY_MODEL_cff6b444001e40bab72876f1b4721a16","value":"Epoch: 100%"}},"e74fe1bf0db04ea09d6c8dbaa1a50f43":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d811b804a9924cc9a2f8448a79c614e5","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_06a418f5fce643aca851a0c9dbf713c6","value":3}},"a89d1b4320bf471b9eaf573c37bbd2ba":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fa758f1fd0554c97bbd3a69210e75a0f","placeholder":"​","style":"IPY_MODEL_c0a26781443a49b88377138261d9aafe","value":" 3/3 [2:34:04&lt;00:00, 3075.15s/it]"}},"1b3c191b4c534509a4c6896ef30e3c84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2615b7f9d548c0b5c2e92af44d02e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cff6b444001e40bab72876f1b4721a16":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d811b804a9924cc9a2f8448a79c614e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"06a418f5fce643aca851a0c9dbf713c6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"fa758f1fd0554c97bbd3a69210e75a0f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0a26781443a49b88377138261d9aafe":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"637051713ddd4ba59a2c1c59164bb00d":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d6619a9fb81149b78d57e6e1f27cf932","IPY_MODEL_74399f7060f5475388164cfb084b92e3","IPY_MODEL_101e18952ab6418e93846f9247a02e2c"],"layout":"IPY_MODEL_5d9927a058c44286886ab0b71624de4c"}},"d6619a9fb81149b78d57e6e1f27cf932":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ae53461ad6e4465ac186dd476f672a6","placeholder":"​","style":"IPY_MODEL_4083711a3aa344a7a0840b5188cf6c4a","value":"Iteration: 100%"}},"74399f7060f5475388164cfb084b92e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_369cbfd1a5b14350a12c04afe05c7f37","max":12239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fcdd84f59d5245a8801dc080cc6e55b3","value":12239}},"101e18952ab6418e93846f9247a02e2c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d8b0ba782744432838aaef86435779a","placeholder":"​","style":"IPY_MODEL_a3436aa78e09403fa847249c2bc45781","value":" 12239/12239 [51:38&lt;00:00,  3.90it/s]"}},"5d9927a058c44286886ab0b71624de4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ae53461ad6e4465ac186dd476f672a6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4083711a3aa344a7a0840b5188cf6c4a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"369cbfd1a5b14350a12c04afe05c7f37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcdd84f59d5245a8801dc080cc6e55b3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4d8b0ba782744432838aaef86435779a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a3436aa78e09403fa847249c2bc45781":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a736ffb9417542e080159ceb07add790":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3a749a15eaa84c73bba4804b74ef718f","IPY_MODEL_f072d9c8ab3c454f95cf464ff0de9af9","IPY_MODEL_667599c4fd1745a2a0c599a90e3ca48a"],"layout":"IPY_MODEL_1378e472b70a46fe91ec7bf15f442621"}},"3a749a15eaa84c73bba4804b74ef718f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbe206598a114a3082fe738ff08ea0e1","placeholder":"​","style":"IPY_MODEL_4f4ba55561ee4138ba523da0da187f6d","value":"Iteration: 100%"}},"f072d9c8ab3c454f95cf464ff0de9af9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ba35bebbc141e0a053a38c82d7a7b3","max":12239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39cad352cb8d4e26979d3dc512930de9","value":12239}},"667599c4fd1745a2a0c599a90e3ca48a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_020ab55170284b3a98b7e650b382a8bf","placeholder":"​","style":"IPY_MODEL_3dfda99a669f4c5a9f8c4fe4c47c6781","value":" 12239/12239 [51:38&lt;00:00,  3.86it/s]"}},"1378e472b70a46fe91ec7bf15f442621":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbe206598a114a3082fe738ff08ea0e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4f4ba55561ee4138ba523da0da187f6d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ba35bebbc141e0a053a38c82d7a7b3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39cad352cb8d4e26979d3dc512930de9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"020ab55170284b3a98b7e650b382a8bf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3dfda99a669f4c5a9f8c4fe4c47c6781":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8cb600570294f5d96f514481b639a48":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_9debc64ca43b44ab95be55ee60a8a3ef","IPY_MODEL_f901412fcb8a4626b2b50372191fa3e3","IPY_MODEL_dac64d578ed5444f96836316b52bcfa9"],"layout":"IPY_MODEL_eab5e3ad03cd4de8a0d05379649a527c"}},"9debc64ca43b44ab95be55ee60a8a3ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d1d6678791b7442582fe062fa8dc438b","placeholder":"​","style":"IPY_MODEL_8d44b18ec77f4e5c8aafe8d25e74e735","value":"Iteration: 100%"}},"f901412fcb8a4626b2b50372191fa3e3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_ac70f69117584e46ae587ed541cf1480","max":12239,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fbf4a20f9ae49c3a2ef556d8fff343d","value":12239}},"dac64d578ed5444f96836316b52bcfa9":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cc3675bdad9d4ec4aad71a06643562df","placeholder":"​","style":"IPY_MODEL_d9a14e6bdb814edcb63e5cfd593eeb59","value":" 12239/12239 [50:47&lt;00:00,  4.45it/s]"}},"eab5e3ad03cd4de8a0d05379649a527c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d1d6678791b7442582fe062fa8dc438b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d44b18ec77f4e5c8aafe8d25e74e735":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac70f69117584e46ae587ed541cf1480":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fbf4a20f9ae49c3a2ef556d8fff343d":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc3675bdad9d4ec4aad71a06643562df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d9a14e6bdb814edcb63e5cfd593eeb59":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a70d223e1274bb88ffcaae969dcfceb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_11ee581e456d4c2ba149a1c473fad0b7","IPY_MODEL_e36093c154394a91991fdbda7650f334","IPY_MODEL_4e861981c6234402b94b23f3500d6e5c"],"layout":"IPY_MODEL_e8cdd6a88de54626a38f527ac65291cf"}},"11ee581e456d4c2ba149a1c473fad0b7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cbac874c6b474a7aa1b7bbd8ba08b422","placeholder":"​","style":"IPY_MODEL_a0fc8b7fbae745be983d93f3224162dc","value":"Evaluating:   0%"}},"e36093c154394a91991fdbda7650f334":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_3697d8fec2e142b0bb8eef251dfa323c","max":1360,"min":0,"orientation":"horizontal","style":"IPY_MODEL_869100ae29404e59814149d3ace1dd75","value":0}},"4e861981c6234402b94b23f3500d6e5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b65db763f4314f04b9138ec00755244b","placeholder":"​","style":"IPY_MODEL_58d6c0282dd04642a0452faa5cf0daf7","value":" 0/1360 [00:00&lt;?, ?it/s]"}},"e8cdd6a88de54626a38f527ac65291cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cbac874c6b474a7aa1b7bbd8ba08b422":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0fc8b7fbae745be983d93f3224162dc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3697d8fec2e142b0bb8eef251dfa323c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"869100ae29404e59814149d3ace1dd75":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b65db763f4314f04b9138ec00755244b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58d6c0282dd04642a0452faa5cf0daf7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":3219644,"sourceType":"datasetVersion","datasetId":1302156}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 🧠 Building a SheldonBot: A Big Bang Theory Chatbot with NLP & DialoGPT\n\nI'm a huge fan of *The Big Bang Theory* and Natural Language Processing (NLP). After watching the final episode of Sheldon and Amy’s Nobel Prize journey, I had an epiphany—it wasn’t just coincidence. I had to combine my love for the show and my skills in NLP and Machine Learning to bring my own **SheldonBot** to life (for now, in chatbot form).\n\n![Sheldon thinking deeply](https://media0.giphy.com/media/v1.Y2lkPTc5MGI3NjExZWZuZzl4Ym5zNHMxeDgyam9qOGVmZHBqM2M3ZGtpYzJzMXFycjBocCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/f79OYWh5uwIfK/giphy.gif)\n\n---\n\nThe road to creating *virtual Sheldon* wasn’t easy. I experimented with several models and approaches, but at first SheldonBot was as socially reluctant as the real Sheldon. His first reply to “Hi” was:\n\n> **\"You're in my spot.\"**\n\n(For context, that was the **only phrase** he knew at the beginning!)\n\nBut I didn't give up. Thanks to the powerful 🤗 Transformers library and fine-tuning Microsoft's DialoGPT model using the **Big Bang Theory Dialogues** dataset from Kaggle, I finally had my breakthrough. SheldonBot and I started having quirky, delightful conversations.\n\n---\n\n## 💬 Sample Conversation\n\n```plaintext\nUser: Hey, Sheldon, want to build a quantum computer together?  \nSheldonBot: Only if I get to name it “Quanteen.”  \n\nUser: That's actually pretty clever  \nSheldonBot: Of course it is. I have an IQ of 187.  \n\nUser: Can we use it to travel through time?  \nSheldonBot: Time travel is theoretically possible but not advisable. Just ask Howard after his zero-gravity toilet incident.  \n\nUser: I want to visit the Renaissance  \nSheldonBot: Fascinating choice. I’d suggest packing Purell and avoiding Galileo’s house—he’s touchy about the heliocentric model.\n","metadata":{"id":"78afhsngLZMw"}},{"cell_type":"markdown","source":"## A bit of theory","metadata":{"id":"yGJHOZYOWZHR"}},{"cell_type":"markdown","source":"# 🤖 How to Create a Virtual Sheldon Using NLP and Transformers\n\nIn this article, I’ll show you how to create a virtual character whose responses are based on the transcript of my favorite science-themed sitcom — *The Big Bang Theory*. You’re welcome to use characters or shows of your own choice, but for this tutorial, I’ll walk you through how I turned Sheldon Cooper into a chatbot.\n\nI also added the code to convert a simple text file of dialogues into a format that the model understands — no need for complex data labeling.\n\n---\n\nTo accomplish this, we’ll use the amazing [Transformers](https://huggingface.co/transformers/) library by Hugging Face. This library hosts the latest state-of-the-art NLP models, including:\n\n- [BERT](https://huggingface.co/transformers/model_doc/bert.html)  \n- [XLNet](https://huggingface.co/transformers/model_doc/xlnet.html)  \n- [GPT-2](https://huggingface.co/transformers/model_doc/gpt2.html)  \n\nIf you're curious about how transformers work, check out Jay Alammar’s beautifully illustrated [article on transformers](http://jalammar.github.io/illustrated-transformer/).\n\n![Transformer model](http://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)  \nImage from [jalammar.github.io](http://jalammar.github.io/illustrated-transformer/)\n\n---\n\n## 🎯 Why DialoGPT?\n\nRecently, Microsoft’s [DialoGPT](https://huggingface.co/transformers/model_doc/dialogpt.html) was added to the Hugging Face model hub. DialoGPT is a variant of GPT-2, trained on 147 million Reddit multi-turn conversation threads. You can learn more about GPT-2 in [this article](http://jalammar.github.io/illustrated-gpt2/).\n\nDialoGPT is particularly well-suited for building chatbots based on TV scripts. It understands dialogue structure and can maintain context across multiple turns — even in its small version. This makes it ideal for bringing a character like Sheldon to life in a *Big Bang Theory*–style conversation.\n\n---\n\n> *Fun fact:* When I first trained SheldonBot, the only thing it would reply with was “You're in my spot.” But with a bit of fine-tuning, things got *Bazinga*-level good.\n\n---\n\nNext, we’ll go step-by-step through:\n- Loading your own dialogue file (we'll use a Big Bang Theory script from Kaggle)\n- Formatting it properly for training\n- Fine-tuning DialoGPT\n- Talking with your virtual Sheldon\n\nReady to build your own sitcom AI? Let’s do this! 🧠💬\n","metadata":{"id":"YBd8PVGNWgfK"}},{"cell_type":"markdown","source":"## First dialogue with DialoGPT","metadata":{"id":"4hyTyW7Le2Sb"}},{"cell_type":"markdown","source":"We will conduct all our experiments in Google Colab / Kaggle, its resources are enough to train the small DialoGPT model.","metadata":{"id":"HEqpXb4ofFGd"}},{"cell_type":"code","source":"! pip -q install transformers","metadata":{"id":"onU41i8g1J3M","executionInfo":{"status":"ok","timestamp":1746435332557,"user_tz":-60,"elapsed":2167,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:10.488605Z","iopub.execute_input":"2025-05-06T13:23:10.488918Z","iopub.status.idle":"2025-05-06T13:23:14.563097Z","shell.execute_reply.started":"2025-05-06T13:23:10.488893Z","shell.execute_reply":"2025-05-06T13:23:14.561961Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"Try to chat with DialoGPT without fine-tuning.","metadata":{"id":"zVSTlysV2jaM"}},{"cell_type":"code","source":"from transformers import AutoModelWithLMHead, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelWithLMHead.from_pretrained(\"microsoft/DialoGPT-small\")","metadata":{"id":"w6qrl7_SvPKg","executionInfo":{"status":"ok","timestamp":1746435334045,"user_tz":-60,"elapsed":1473,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"b61a12ed-bd58-4255-e822-214482106b31","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:14.564735Z","iopub.execute_input":"2025-05-06T13:23:14.565016Z","iopub.status.idle":"2025-05-06T13:23:35.605870Z","shell.execute_reply.started":"2025-05-06T13:23:14.564995Z","shell.execute_reply":"2025-05-06T13:23:35.605004Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/641 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca9082320c3b4ed8b780178dbc21b6ed"}},"metadata":{}},{"name":"stderr","text":"2025-05-06 13:23:20.980013: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746537801.231355      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746537801.300928      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/351M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9fa74d5e7d5547d5806491b58d9a04f8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"28ce9304cae84e9785a1616e4b173951"}},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-small\")\nmodel = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-small\")\n\nprompts = [\"Hello, how are you?\", \"What can you do?\"]\nchat_history_ids = None\n\nfor step in range(2):\n    new_user_input_ids = tokenizer.encode(prompts[step] + tokenizer.eos_token, return_tensors='pt')\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=1000,\n        pad_token_id=tokenizer.eos_token_id\n    )\n\n    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n    print(f\"DialoGPT: {response}\")\n","metadata":{"id":"CjZaN5ilgd-z","executionInfo":{"status":"ok","timestamp":1746435362975,"user_tz":-60,"elapsed":28930,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"22af872b-ce3b-4fad-91a9-6c2c07c49ce9","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:35.606690Z","iopub.execute_input":"2025-05-06T13:23:35.607145Z","iopub.status.idle":"2025-05-06T13:23:37.165579Z","shell.execute_reply.started":"2025-05-06T13:23:35.607126Z","shell.execute_reply":"2025-05-06T13:23:37.164847Z"}},"outputs":[{"name":"stderr","text":"The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"DialoGPT: Hi, I'm a guy.\nDialoGPT: I can do this.\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"![alt text](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExNGdrbWkzYmFld3V5MXkzZ3YzcjE1NzVkcjN6Ymkyemc1b2hqbjdzaSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/xT0xehbY7qJnF4xv8Y/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)\n\nThat's not quite good answers!. We will fix it with fine-tuning.","metadata":{"id":"EAy2OBMW6QyD"}},{"cell_type":"markdown","source":"## Model initial configuration","metadata":{"id":"IuzSROqxjUKM"}},{"cell_type":"markdown","source":"Let's train our own SheldonBot. For start, we will need basic configuration and a dataset.\nConfiguration and training scripts are mostly based on this [script](https://github.com/huggingface/transformers/blob/master/examples/language-modeling/run_language_modeling.py) from Huggingface and great [tutorial](https://nathancooper.io/i-am-a-nerd/chatbot/deep-learning/gpt2/2020/05/12/chatbot-part-1.html) from Nathan Cooper.","metadata":{"id":"TC3qNlfp30aU"}},{"cell_type":"code","source":"\"\"\"\nFine-tuning Hugging Face models for language modeling on a text file (GPT, GPT-2, BERT, RoBERTa).\nGPT/GPT-2 use causal language modeling (CLM); BERT/RoBERTa use masked language modeling (MLM).\n\"\"\"\n\nimport os\nimport re\nimport glob\nimport pickle\nimport random\nimport shutil\nimport logging\nfrom pathlib import Path\nfrom typing import Dict, List, Tuple\n\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset, RandomSampler, SequentialSampler\nfrom torch.nn.utils.rnn import pad_sequence\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm, trange\nfrom torch.optim import AdamW\nfrom transformers import (\n    WEIGHTS_NAME,\n    AutoConfig,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    AutoModelForCausalLM,      # For GPT, GPT-2\n    AutoModelForMaskedLM       # For BERT, RoBERTa\n)\n\n\n# TensorBoard support\ntry:\n    from torch.utils.tensorboard import SummaryWriter\nexcept ImportError:\n    from tensorboardX import SummaryWriter\n\n# Setup logging\nlogger = logging.getLogger(__name__)\nlogging.basicConfig(level=logging.INFO)\n\n# Define supported model types\nMODEL_TYPES = (\n    \"gpt2\",         # causal LM\n    \"bert\",         # masked LM\n    \"roberta\",      # masked LM\n    \"distilbert\",   # masked LM\n    \"openai-gpt\",   # causal LM\n    \"xlnet\",        # permutation LM\n    \"ctrl\",         # conditional LM\n    \"transfo-xl\",   # transformer-XL LM\n    \"xlm\",          # multilingual LM\n)\n","metadata":{"id":"g91QzdqU2haO","executionInfo":{"status":"ok","timestamp":1746435362978,"user_tz":-60,"elapsed":2,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:37.168158Z","iopub.execute_input":"2025-05-06T13:23:37.168391Z","iopub.status.idle":"2025-05-06T13:23:37.590763Z","shell.execute_reply.started":"2025-05-06T13:23:37.168366Z","shell.execute_reply":"2025-05-06T13:23:37.590237Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"# Args to allow for easy convertion of python script to notebook\nclass Args():\n    def __init__(self):\n        self.output_dir = 'output-small'\n        self.model_type = 'gpt2'\n        self.model_name_or_path = 'microsoft/DialoGPT-small'\n        self.config_name = 'microsoft/DialoGPT-small'\n        self.tokenizer_name = 'microsoft/DialoGPT-small'\n        self.cache_dir = 'cached'\n        self.block_size = 512\n        self.do_train = True\n        self.do_eval = True\n        self.evaluate_during_training = False\n        self.per_gpu_train_batch_size = 4\n        self.per_gpu_eval_batch_size = 4\n        self.gradient_accumulation_steps = 1\n        self.learning_rate = 5e-5\n        self.weight_decay = 0.0\n        self.adam_epsilon = 1e-8\n        self.max_grad_norm = 1.0\n        self.num_train_epochs = 3\n        self.max_steps = -1\n        self.warmup_steps = 0\n        self.logging_steps = 1000\n        self.save_steps = 3500\n        self.save_total_limit = None\n        self.eval_all_checkpoints = False\n        self.no_cuda = False\n        self.overwrite_output_dir = True\n        self.overwrite_cache = True\n        self.should_continue = False\n        self.seed = 42\n        self.local_rank = -1\n        self.fp16 = False\n        self.fp16_opt_level = 'O1'\n\nargs = Args()","metadata":{"id":"utprDGf06OVt","executionInfo":{"status":"ok","timestamp":1746435362992,"user_tz":-60,"elapsed":8,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:37.591566Z","iopub.execute_input":"2025-05-06T13:23:37.591869Z","iopub.status.idle":"2025-05-06T13:23:37.597531Z","shell.execute_reply.started":"2025-05-06T13:23:37.591850Z","shell.execute_reply":"2025-05-06T13:23:37.596962Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"## Prepare Dataset","metadata":{"id":"X_qYqlTe9yx2"}},{"cell_type":"markdown","source":"# 🧠 The Big Bang Theory Bot - Dataset Setup\n\n## Get the data set from kaggle\nYou can use the code from the dataset source in kaggle [https://www.kaggle.com/code/lydia70/big-bang-theory-tv-show]","metadata":{"id":"uo718Zb-961e"}},{"cell_type":"code","source":"import kagglehub\nthe_big_bang_theory_series_transcript_path = kagglehub.dataset_download('mitramir5/the-big-bang-theory-series-transcript')\nprint('Data source import complete.')","metadata":{"id":"r-HczUyTnyzh","executionInfo":{"status":"ok","timestamp":1746435384755,"user_tz":-60,"elapsed":1817,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"666beabe-d4cc-43d7-c708-76c2ae8fc35f","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:37.598330Z","iopub.execute_input":"2025-05-06T13:23:37.598582Z","iopub.status.idle":"2025-05-06T13:23:37.903927Z","shell.execute_reply.started":"2025-05-06T13:23:37.598561Z","shell.execute_reply":"2025-05-06T13:23:37.903163Z"}},"outputs":[{"name":"stdout","text":"Data source import complete.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","metadata":{"id":"ivUgcYk5r_Ff","executionInfo":{"status":"ok","timestamp":1746435384760,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"c9fd609c-5809-41a6-b0b2-be075df2215f","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:37.904743Z","iopub.execute_input":"2025-05-06T13:23:37.905033Z","iopub.status.idle":"2025-05-06T13:23:37.914349Z","shell.execute_reply.started":"2025-05-06T13:23:37.905010Z","shell.execute_reply":"2025-05-06T13:23:37.913844Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/the-big-bang-theory-series-transcript/sentences_sentiment_dicts.pkl\n/kaggle/input/the-big-bang-theory-series-transcript/1_10_seasons_tbbt.csv\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"#import dataset\npath = '/kaggle/input/the-big-bang-theory-series-transcript/1_10_seasons_tbbt.csv'\ndf = pd.read_csv(path)\ndf.head()","metadata":{"id":"TxHdyv67dptm","executionInfo":{"status":"ok","timestamp":1746435384940,"user_tz":-60,"elapsed":171,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"7554cc16-399f-46f8-a16e-1a2ee9118746","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:37.915291Z","iopub.execute_input":"2025-05-06T13:23:37.915560Z","iopub.status.idle":"2025-05-06T13:23:38.140231Z","shell.execute_reply.started":"2025-05-06T13:23:37.915536Z","shell.execute_reply":"2025-05-06T13:23:38.139397Z"}},"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"                           episode_name  \\\n0  Series 01 Episode 01 – Pilot Episode   \n1  Series 01 Episode 01 – Pilot Episode   \n2  Series 01 Episode 01 – Pilot Episode   \n3  Series 01 Episode 01 – Pilot Episode   \n4  Series 01 Episode 01 – Pilot Episode   \n\n                                            dialogue person_scene  \n0                        A corridor at a sperm bank.        Scene  \n1   So if a photon is directed through a plane wi...      Sheldon  \n2                         Agreed, what’s your point?      Leonard  \n3   There’s no point, I just think it’s a good id...      Sheldon  \n4                                         Excuse me?      Leonard  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>episode_name</th>\n      <th>dialogue</th>\n      <th>person_scene</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Series 01 Episode 01 – Pilot Episode</td>\n      <td>A corridor at a sperm bank.</td>\n      <td>Scene</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Series 01 Episode 01 – Pilot Episode</td>\n      <td>So if a photon is directed through a plane wi...</td>\n      <td>Sheldon</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Series 01 Episode 01 – Pilot Episode</td>\n      <td>Agreed, what’s your point?</td>\n      <td>Leonard</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Series 01 Episode 01 – Pilot Episode</td>\n      <td>There’s no point, I just think it’s a good id...</td>\n      <td>Sheldon</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Series 01 Episode 01 – Pilot Episode</td>\n      <td>Excuse me?</td>\n      <td>Leonard</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"We will convert this dataset in a way that every responce row will contain **n** previous responces as a context. For our purposes seven previous responces will be enough.","metadata":{"id":"n51hsl2mAG8v"}},{"cell_type":"code","source":"contexted = []\nn = 7\nfor i in range(n, len(df['dialogue'])):\n  row = []\n  prev = i - 1 - n # we additionally substract 1, so row will contain current responce and 7 previous responces\n  for j in range(i, prev, -1):\n    row.append(df['dialogue'][j])\n  contexted.append(row)","metadata":{"id":"WyL7Dx1bbLRC","executionInfo":{"status":"ok","timestamp":1746435389078,"user_tz":-60,"elapsed":4140,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:38.141168Z","iopub.execute_input":"2025-05-06T13:23:38.141480Z","iopub.status.idle":"2025-05-06T13:23:39.797371Z","shell.execute_reply.started":"2025-05-06T13:23:38.141451Z","shell.execute_reply":"2025-05-06T13:23:39.796764Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"len(contexted)","metadata":{"id":"R_cwbBolVi14","executionInfo":{"status":"ok","timestamp":1746435389089,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"167d899d-08b0-4293-d72e-d9631efb11e6","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.799273Z","iopub.execute_input":"2025-05-06T13:23:39.799471Z","iopub.status.idle":"2025-05-06T13:23:39.804325Z","shell.execute_reply.started":"2025-05-06T13:23:39.799456Z","shell.execute_reply":"2025-05-06T13:23:39.803754Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"54399"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"columns = ['response', 'context']\ncolumns = columns + ['context/'+str(i) for i in range(n-1)]\ncolumns","metadata":{"id":"gNkYLty-VhTO","executionInfo":{"status":"ok","timestamp":1746435389109,"user_tz":-60,"elapsed":19,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"3352e944-1629-4e77-c85e-ae1303eee5fb","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.805132Z","iopub.execute_input":"2025-05-06T13:23:39.805387Z","iopub.status.idle":"2025-05-06T13:23:39.822714Z","shell.execute_reply.started":"2025-05-06T13:23:39.805371Z","shell.execute_reply":"2025-05-06T13:23:39.821943Z"}},"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"['response',\n 'context',\n 'context/0',\n 'context/1',\n 'context/2',\n 'context/3',\n 'context/4',\n 'context/5']"},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"df = pd.DataFrame.from_records(contexted, columns=columns)\ndf.head(5)","metadata":{"id":"kPafxqIYgurW","executionInfo":{"status":"ok","timestamp":1746435389637,"user_tz":-60,"elapsed":526,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"019cd256-a0cf-4e30-8dcc-c31dd5155001","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.823397Z","iopub.execute_input":"2025-05-06T13:23:39.823597Z","iopub.status.idle":"2025-05-06T13:23:39.858674Z","shell.execute_reply.started":"2025-05-06T13:23:39.823565Z","shell.execute_reply":"2025-05-06T13:23:39.858162Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"                                            response  \\\n0                                    Can I help you?   \n1           Yes. Um, is this the High IQ sperm bank?   \n2   If you have to ask, maybe you shouldn’t be here.   \n3                         I think this is the place.   \n4                                    Fill these out.   \n\n                                             context  \\\n0   One across is Aegean, eight down is Nabakov, ...   \n1                                    Can I help you?   \n2           Yes. Um, is this the High IQ sperm bank?   \n3   If you have to ask, maybe you shouldn’t be here.   \n4                         I think this is the place.   \n\n                                           context/0  \\\n0                                          Hang on.    \n1   One across is Aegean, eight down is Nabakov, ...   \n2                                    Can I help you?   \n3           Yes. Um, is this the High IQ sperm bank?   \n4   If you have to ask, maybe you shouldn’t be here.   \n\n                                           context/1  \\\n0                                         Excuse me?   \n1                                          Hang on.    \n2   One across is Aegean, eight down is Nabakov, ...   \n3                                    Can I help you?   \n4           Yes. Um, is this the High IQ sperm bank?   \n\n                                           context/2  \\\n0   There’s no point, I just think it’s a good id...   \n1                                         Excuse me?   \n2                                          Hang on.    \n3   One across is Aegean, eight down is Nabakov, ...   \n4                                    Can I help you?   \n\n                                           context/3  \\\n0                         Agreed, what’s your point?   \n1   There’s no point, I just think it’s a good id...   \n2                                         Excuse me?   \n3                                          Hang on.    \n4   One across is Aegean, eight down is Nabakov, ...   \n\n                                           context/4  \\\n0   So if a photon is directed through a plane wi...   \n1                         Agreed, what’s your point?   \n2   There’s no point, I just think it’s a good id...   \n3                                         Excuse me?   \n4                                          Hang on.    \n\n                                           context/5  \n0                        A corridor at a sperm bank.  \n1   So if a photon is directed through a plane wi...  \n2                         Agreed, what’s your point?  \n3   There’s no point, I just think it’s a good id...  \n4                                         Excuse me?  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Can I help you?</td>\n      <td>One across is Aegean, eight down is Nabakov, ...</td>\n      <td>Hang on.</td>\n      <td>Excuse me?</td>\n      <td>There’s no point, I just think it’s a good id...</td>\n      <td>Agreed, what’s your point?</td>\n      <td>So if a photon is directed through a plane wi...</td>\n      <td>A corridor at a sperm bank.</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Yes. Um, is this the High IQ sperm bank?</td>\n      <td>Can I help you?</td>\n      <td>One across is Aegean, eight down is Nabakov, ...</td>\n      <td>Hang on.</td>\n      <td>Excuse me?</td>\n      <td>There’s no point, I just think it’s a good id...</td>\n      <td>Agreed, what’s your point?</td>\n      <td>So if a photon is directed through a plane wi...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>If you have to ask, maybe you shouldn’t be here.</td>\n      <td>Yes. Um, is this the High IQ sperm bank?</td>\n      <td>Can I help you?</td>\n      <td>One across is Aegean, eight down is Nabakov, ...</td>\n      <td>Hang on.</td>\n      <td>Excuse me?</td>\n      <td>There’s no point, I just think it’s a good id...</td>\n      <td>Agreed, what’s your point?</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>I think this is the place.</td>\n      <td>If you have to ask, maybe you shouldn’t be here.</td>\n      <td>Yes. Um, is this the High IQ sperm bank?</td>\n      <td>Can I help you?</td>\n      <td>One across is Aegean, eight down is Nabakov, ...</td>\n      <td>Hang on.</td>\n      <td>Excuse me?</td>\n      <td>There’s no point, I just think it’s a good id...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Fill these out.</td>\n      <td>I think this is the place.</td>\n      <td>If you have to ask, maybe you shouldn’t be here.</td>\n      <td>Yes. Um, is this the High IQ sperm bank?</td>\n      <td>Can I help you?</td>\n      <td>One across is Aegean, eight down is Nabakov, ...</td>\n      <td>Hang on.</td>\n      <td>Excuse me?</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"markdown","source":"Split our dataset into a training and test parts.","metadata":{"id":"aBeM8pvEjigq"}},{"cell_type":"code","source":"trn_df, val_df = train_test_split(df, test_size = 0.1)\ntrn_df.head()","metadata":{"id":"g1CeutVVlL85","executionInfo":{"status":"ok","timestamp":1746435390152,"user_tz":-60,"elapsed":511,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"outputId":"82d14017-7f82-4947-c93e-1ee2a4d820f6","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.859345Z","iopub.execute_input":"2025-05-06T13:23:39.859550Z","iopub.status.idle":"2025-05-06T13:23:39.881065Z","shell.execute_reply.started":"2025-05-06T13:23:39.859534Z","shell.execute_reply":"2025-05-06T13:23:39.880435Z"}},"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"                                                response  \\\n51043   Now I see three of you. Do we know the father...   \n6443                                               What?   \n53038                    That would be great. Thank you.   \n43244                          We are going to Me-hi-co.   \n12317   Thankfully, my shock was somewhat mitigated b...   \n\n                                                 context  \\\n51043                                  A delivery room.    \n6443    She’s sending virtual livestock to random men...   \n53038   Don’t worry about it. Hey, after this, how ab...   \n43244                                        Your house?   \n12317                                        Sure, sure.   \n\n                                               context/0  \\\n51043   Yeah. I’m afraid so. Childbirth, looming coit...   \n6443                He’s having problems with Stephanie?   \n53038               I’d still like to know who Jerry is.   \n43244                                We can drive there.   \n12317   Well, initially I felt something akin to grie...   \n\n                                               context/1  \\\n51043                   I guess, I guess we should stop.   \n6443    I’m certain this will come as no surprise to ...   \n53038     I thought someone looked brighter and tighter.   \n43244                                             India.   \n12317   Come on, I mean, you’re not upset that your m...   \n\n                                               context/2  \\\n51043              Come on, we’re going to the hospital.   \n6443                          Sheldon, what do you want?   \n53038   Hey, I hope you don’t mind, I used a little o...   \n43244   Uh, okay, let’s see. They’ve got spicy food a...   \n12317   Oh, not at all. But I am a bit distressed to ...   \n\n                                               context/3  \\\n51043       Guys, wake up. Bernadette’s having her baby.   \n6443                                How can you be sure?   \n53038   Oh, well, then I can actually feel the moistu...   \n43244   So, are you gonna give us a clue where we’re ...   \n12317   Yeah, I gotta put a sticker over that. So, uh...   \n\n                                               context/4  \\\n51043   You hate Scotch tape, but you love Scotch pla...   \n6443                                                 No.   \n53038                 Well, this is a moisturizing mask.   \n43244          Yeah, and talked about physics with them.   \n12317                     Your check engine light is on.   \n\n                                               context/5  \n51043   I didn’t put on my come-hither plaid PJ’s for...  \n6443    I suppose I could spare a few minutes. Were y...  \n53038   I can actually feel the toxins being pulled o...  \n43244   I bet he picked up a lot of cute grad student...  \n12317                                    No, you do not.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>response</th>\n      <th>context</th>\n      <th>context/0</th>\n      <th>context/1</th>\n      <th>context/2</th>\n      <th>context/3</th>\n      <th>context/4</th>\n      <th>context/5</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>51043</th>\n      <td>Now I see three of you. Do we know the father...</td>\n      <td>A delivery room.</td>\n      <td>Yeah. I’m afraid so. Childbirth, looming coit...</td>\n      <td>I guess, I guess we should stop.</td>\n      <td>Come on, we’re going to the hospital.</td>\n      <td>Guys, wake up. Bernadette’s having her baby.</td>\n      <td>You hate Scotch tape, but you love Scotch pla...</td>\n      <td>I didn’t put on my come-hither plaid PJ’s for...</td>\n    </tr>\n    <tr>\n      <th>6443</th>\n      <td>What?</td>\n      <td>She’s sending virtual livestock to random men...</td>\n      <td>He’s having problems with Stephanie?</td>\n      <td>I’m certain this will come as no surprise to ...</td>\n      <td>Sheldon, what do you want?</td>\n      <td>How can you be sure?</td>\n      <td>No.</td>\n      <td>I suppose I could spare a few minutes. Were y...</td>\n    </tr>\n    <tr>\n      <th>53038</th>\n      <td>That would be great. Thank you.</td>\n      <td>Don’t worry about it. Hey, after this, how ab...</td>\n      <td>I’d still like to know who Jerry is.</td>\n      <td>I thought someone looked brighter and tighter.</td>\n      <td>Hey, I hope you don’t mind, I used a little o...</td>\n      <td>Oh, well, then I can actually feel the moistu...</td>\n      <td>Well, this is a moisturizing mask.</td>\n      <td>I can actually feel the toxins being pulled o...</td>\n    </tr>\n    <tr>\n      <th>43244</th>\n      <td>We are going to Me-hi-co.</td>\n      <td>Your house?</td>\n      <td>We can drive there.</td>\n      <td>India.</td>\n      <td>Uh, okay, let’s see. They’ve got spicy food a...</td>\n      <td>So, are you gonna give us a clue where we’re ...</td>\n      <td>Yeah, and talked about physics with them.</td>\n      <td>I bet he picked up a lot of cute grad student...</td>\n    </tr>\n    <tr>\n      <th>12317</th>\n      <td>Thankfully, my shock was somewhat mitigated b...</td>\n      <td>Sure, sure.</td>\n      <td>Well, initially I felt something akin to grie...</td>\n      <td>Come on, I mean, you’re not upset that your m...</td>\n      <td>Oh, not at all. But I am a bit distressed to ...</td>\n      <td>Yeah, I gotta put a sticker over that. So, uh...</td>\n      <td>Your check engine light is on.</td>\n      <td>No, you do not.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":17},{"cell_type":"markdown","source":"Now will convert our dataset in a format suitable for our model. Basically we will concatenate responses in one string for each row (additionally we will add special 'end of string' token between responses, so the model will understand end of each response in a string).  ","metadata":{"id":"86F3WhnFO4H8"}},{"cell_type":"code","source":"from transformers import PreTrainedTokenizer\n\ndef construct_conv(row, tokenizer, eos=True):\n    flatten = lambda l: [item for sublist in l for item in sublist]\n    conv = list(\n        reversed([\n            tokenizer.encode(str(x)) + [tokenizer.eos_token_id]\n            for x in row if pd.notna(x)\n        ])\n    )\n    return flatten(conv)\n\n\nclass ConversationDataset(Dataset):\n    def __init__(self, tokenizer: PreTrainedTokenizer, args, df, block_size=512):\n\n        block_size = block_size - (tokenizer.model_max_length - tokenizer.max_len_single_sentence)\n\n        directory = args.cache_dir\n        cached_features_file = os.path.join(\n            directory, args.model_type + \"_cached_lm_\" + str(block_size)\n        )\n\n        if os.path.exists(cached_features_file) and not args.overwrite_cache:\n            logger.info(\"Loading features from cached file %s\", cached_features_file)\n            with open(cached_features_file, \"rb\") as handle:\n                self.examples = pickle.load(handle)\n        else:\n            logger.info(\"Creating features from dataset file at %s\", directory)\n\n            self.examples = []\n            for _, row in df.iterrows():\n                conv = construct_conv(row, tokenizer)\n                self.examples.append(conv)\n\n            logger.info(\"Saving features into cached file %s\", cached_features_file)\n            with open(cached_features_file, \"wb\") as handle:\n                pickle.dump(self.examples, handle, protocol=pickle.HIGHEST_PROTOCOL)\n\n    def __len__(self):\n        return len(self.examples)\n\n    def __getitem__(self, item):\n        return torch.tensor(self.examples[item], dtype=torch.long)","metadata":{"id":"PX7jeWpYmOe_","executionInfo":{"status":"ok","timestamp":1746435390159,"user_tz":-60,"elapsed":4,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.881729Z","iopub.execute_input":"2025-05-06T13:23:39.881978Z","iopub.status.idle":"2025-05-06T13:23:39.889141Z","shell.execute_reply.started":"2025-05-06T13:23:39.881962Z","shell.execute_reply":"2025-05-06T13:23:39.888447Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Cacheing and storing of data/checkpoints\n\ndef load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False):\n    return ConversationDataset(tokenizer, args, df_val if evaluate else df_trn)\n\n\ndef set_seed(args):\n    random.seed(args.seed)\n    np.random.seed(args.seed)\n    torch.manual_seed(args.seed)\n    if args.n_gpu > 0:\n        torch.cuda.manual_seed_all(args.seed)\n\n\ndef _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> List[str]:\n    ordering_and_checkpoint_path = []\n\n    glob_checkpoints = glob.glob(os.path.join(args.output_dir, \"{}-*\".format(checkpoint_prefix)))\n\n    for path in glob_checkpoints:\n        if use_mtime:\n            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n        else:\n            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n            if regex_match and regex_match.groups():\n                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n\n    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n    return checkpoints_sorted\n\n\ndef _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False) -> None:\n    if not args.save_total_limit:\n        return\n    if args.save_total_limit <= 0:\n        return\n\n    # Check if we should delete older checkpoint(s)\n    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n    if len(checkpoints_sorted) <= args.save_total_limit:\n        return\n\n    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args.save_total_limit)\n    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n    for checkpoint in checkpoints_to_be_deleted:\n        logger.info(\"Deleting older checkpoint [{}] due to args.save_total_limit\".format(checkpoint))\n        shutil.rmtree(checkpoint)","metadata":{"id":"naaRHoXgnStq","executionInfo":{"status":"ok","timestamp":1746435390172,"user_tz":-60,"elapsed":10,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.889982Z","iopub.execute_input":"2025-05-06T13:23:39.890216Z","iopub.status.idle":"2025-05-06T13:23:39.909417Z","shell.execute_reply.started":"2025-05-06T13:23:39.890196Z","shell.execute_reply":"2025-05-06T13:23:39.908890Z"}},"outputs":[],"execution_count":19},{"cell_type":"markdown","source":"## Training and Evaluating\n\nThere will be quite a lot of code needed for training our model but don’t worry, everything should work as is, the main thing is to give the model the dataset in the right format.\n\n![alt text](https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExN2Vsenc3b2M2dHhmeXNyY25qNmkzOXoxdGNhcjJhd2JzaGhtaXJtMSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/29SqSyXlyO6WI/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)","metadata":{"id":"pkvMNnrnVHQw"}},{"cell_type":"code","source":"from transformers import PreTrainedModel, PreTrainedTokenizer\ndef train(args, train_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer) -> Tuple[int, float]:\n    \"\"\" Train the model \"\"\"\n    if args.local_rank in [-1, 0]:\n        tb_writer = SummaryWriter()\n\n    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n\n    def collate(examples: List[torch.Tensor]):\n        # Change is here, use tokenizer.pad_token_id instead of tokenizer._pad_token\n        if tokenizer.pad_token_id is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n    train_dataloader = DataLoader(\n        train_dataset, sampler=train_sampler, batch_size=args.train_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    if args.max_steps > 0:\n        t_total = args.max_steps\n        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n    else:\n        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n\n    model = model.module if hasattr(model, \"module\") else model  # Take care of distributed/parallel training\n    model.resize_token_embeddings(len(tokenizer))\n    # add_special_tokens_(model, tokenizer)\n\n\n    # Prepare optimizer and schedule (linear warmup and decay)\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": args.weight_decay,\n        },\n        {\"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \"weight_decay\": 0.0},\n    ]\n    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n    )\n\n    # Check if saved optimizer or scheduler states exist\n    if (\n        args.model_name_or_path\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\"))\n        and os.path.isfile(os.path.join(args.model_name_or_path, \"scheduler.pt\"))\n    ):\n        # Load in optimizer and scheduler states\n        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n\n    if args.fp16:\n        try:\n            from apex import amp\n        except ImportError:\n            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n\n    # multi-gpu training (should be after apex fp16 initialization)\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Distributed training (should be after apex fp16 initialization)\n    if args.local_rank != -1:\n        model = torch.nn.parallel.DistributedDataParallel(\n            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n        )\n\n    # Train!\n    logger.info(\"***** Running training *****\")\n    logger.info(\"  Num examples = %d\", len(train_dataset))\n    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n    logger.info(\"  Instantaneous batch size per GPU = %d\", args.per_gpu_train_batch_size)\n    logger.info(\n        \"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n        args.train_batch_size\n        * args.gradient_accumulation_steps\n        * (torch.distributed.get_world_size() if args.local_rank != -1 else 1),\n    )\n    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n    logger.info(\"  Total optimization steps = %d\", t_total)\n\n    global_step = 0\n    epochs_trained = 0\n    steps_trained_in_current_epoch = 0\n    # Check if continuing training from a checkpoint\n    if args.model_name_or_path and os.path.exists(args.model_name_or_path):\n        try:\n            # set global_step to gobal_step of last saved checkpoint from model path\n            checkpoint_suffix = args.model_name_or_path.split(\"-\")[-1].split(\"/\")[0]\n            global_step = int(checkpoint_suffix)\n            epochs_trained = global_step // (len(train_dataloader) // args.gradient_accumulation_steps)\n            steps_trained_in_current_epoch = global_step % (len(train_dataloader) // args.gradient_accumulation_steps)\n\n            logger.info(\"  Continuing training from checkpoint, will skip to saved global_step\")\n            logger.info(\"  Continuing training from epoch %d\", epochs_trained)\n            logger.info(\"  Continuing training from global step %d\", global_step)\n            logger.info(\"  Will skip the first %d steps in the first epoch\", steps_trained_in_current_epoch)\n        except ValueError:\n            logger.info(\"  Starting fine-tuning.\")\n\n    tr_loss, logging_loss = 0.0, 0.0\n\n    model.zero_grad()\n    train_iterator = trange(\n        epochs_trained, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n    )\n    set_seed(args)  # Added here for reproducibility\n    for _ in train_iterator:\n        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n        for step, batch in enumerate(epoch_iterator):\n\n            # Skip past any already trained steps if resuming training\n            if steps_trained_in_current_epoch > 0:\n                steps_trained_in_current_epoch -= 1\n                continue\n\n            inputs, labels = (batch, batch)\n            if inputs.shape[1] > 1024: continue\n            inputs = inputs.to(args.device)\n            labels = labels.to(args.device)\n            model.train()\n            outputs = model(inputs, labels=labels)\n            loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n\n            if args.n_gpu > 1:\n                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n            if args.gradient_accumulation_steps > 1:\n                loss = loss / args.gradient_accumulation_steps\n\n            if args.fp16:\n                with amp.scale_loss(loss, optimizer) as scaled_loss:\n                    scaled_loss.backward()\n            else:\n                loss.backward()\n\n            tr_loss += loss.item()\n            if (step + 1) % args.gradient_accumulation_steps == 0:\n                if args.fp16:\n                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n                else:\n                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n                optimizer.step()\n                scheduler.step()  # Update learning rate schedule\n                model.zero_grad()\n                global_step += 1\n\n                if args.local_rank in [-1, 0] and args.logging_steps > 0 and global_step % args.logging_steps == 0:\n                    # Log metrics\n                    if (\n                        args.local_rank == -1 and args.evaluate_during_training\n                    ):  # Only evaluate when single GPU otherwise metrics may not average well\n                        results = evaluate(args, model, tokenizer)\n                        for key, value in results.items():\n                            tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n                    tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n                    tb_writer.add_scalar(\"loss\", (tr_loss - logging_loss) / args.logging_steps, global_step)\n                    logging_loss = tr_loss\n\n                if args.local_rank in [-1, 0] and args.save_steps > 0 and global_step % args.save_steps == 0:\n                    checkpoint_prefix = \"checkpoint\"\n                    # Save model checkpoint\n                    output_dir = os.path.join(args.output_dir, \"{}-{}\".format(checkpoint_prefix, global_step))\n                    os.makedirs(output_dir, exist_ok=True)\n                    model_to_save = (\n                        model.module if hasattr(model, \"module\") else model\n                    )  # Take care of distributed/parallel training\n                    model_to_save.save_pretrained(output_dir)\n                    tokenizer.save_pretrained(output_dir)\n\n                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n                    logger.info(\"Saving model checkpoint to %s\", output_dir)\n\n                    _rotate_checkpoints(args, checkpoint_prefix)\n\n                    torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n                    torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n                    logger.info(\"Saving optimizer and scheduler states to %s\", output_dir)\n\n            if args.max_steps > 0 and global_step > args.max_steps:\n                epoch_iterator.close()\n                break\n        if args.max_steps > 0 and global_step > args.max_steps:\n            train_iterator.close()\n            break\n\n    if args.local_rank in [-1, 0]:\n        tb_writer.close()\n\n    return global_step, tr_loss / global_step\n\n# Evaluation of some model\n\ndef evaluate(args, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, df_trn, df_val, prefix=\"\") -> Dict:\n    # Loop to handle MNLI double evaluation (matched, mis-matched)\n    eval_output_dir = args.output_dir\n\n    eval_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=True)\n    os.makedirs(eval_output_dir, exist_ok=True)\n    args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n    # Note that DistributedSampler samples randomly\n\n    def collate(examples: List[torch.Tensor]):\n        # Change is here, use tokenizer.pad_token_id instead of tokenizer._pad_token\n        if tokenizer.pad_token_id is None:\n            return pad_sequence(examples, batch_first=True)\n        return pad_sequence(examples, batch_first=True, padding_value=tokenizer.pad_token_id)\n\n    eval_sampler = SequentialSampler(eval_dataset)\n    eval_dataloader = DataLoader(\n        eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size, collate_fn=collate, drop_last = True\n    )\n\n    # multi-gpu evaluate\n    if args.n_gpu > 1:\n        model = torch.nn.DataParallel(model)\n\n    # Eval!\n    logger.info(\"***** Running evaluation {} *****\".format(prefix))\n    logger.info(\"  Num examples = %d\", len(eval_dataset))\n    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n    eval_loss = 0.0\n    nb_eval_steps = 0\n    model.eval()\n\n    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n        inputs, labels = (batch, batch)\n        inputs = inputs.to(args.device)\n        labels = labels.to(args.device)\n\n        with torch.no_grad():\n            outputs = model(inputs, labels=labels)\n            lm_loss = outputs[0]\n            eval_loss += lm_loss.mean().item()\n        nb_eval_steps += 1\n\n    eval_loss = eval_loss / nb_eval_steps\n    perplexity = torch.exp(torch.tensor(eval_loss))\n\n    result = {\"perplexity\": perplexity}\n\n    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n    with open(output_eval_file, \"w\") as writer:\n        logger.info(\"***** Eval results {} *****\".format(prefix))\n        for key in sorted(result.keys()):\n            logger.info(\"  %s = %s\", key, str(result[key]))\n            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n\n    return result","metadata":{"id":"tXzKlXHeu0Mb","executionInfo":{"status":"ok","timestamp":1746435390225,"user_tz":-60,"elapsed":54,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.910175Z","iopub.execute_input":"2025-05-06T13:23:39.910396Z","iopub.status.idle":"2025-05-06T13:23:39.939437Z","shell.execute_reply.started":"2025-05-06T13:23:39.910373Z","shell.execute_reply":"2025-05-06T13:23:39.938813Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"import os\nimport torch\nimport logging\nfrom transformers import AutoConfig, AutoTokenizer, AutoModelForCausalLM  # Updated import\n\n# Main runner\ndef main(df_trn, df_val):\n    args = Args()\n\n    # Handle checkpoints if continuing training\n    if args.should_continue:\n        sorted_checkpoints = _sorted_checkpoints(args)\n        if len(sorted_checkpoints) == 0:\n            raise ValueError(\"Used --should_continue but no checkpoint was found in --output_dir.\")\n        else:\n            args.model_name_or_path = sorted_checkpoints[-1]\n\n    # Check output directory for training continuation\n    if (\n        os.path.exists(args.output_dir)\n        and os.listdir(args.output_dir)\n        and args.do_train\n        and not args.overwrite_output_dir\n        and not args.should_continue\n    ):\n        raise ValueError(\n            \"Output directory ({}) already exists and is not empty. Use --overwrite_output_dir to overcome.\".format(\n                args.output_dir\n            )\n        )\n\n    # Setup CUDA, GPU & distributed training\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # Updated for Kaggle compatibility\n    args.n_gpu = torch.cuda.device_count() if torch.cuda.is_available() else 0\n    args.device = device\n\n    # Setup logging\n    logging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN,\n    )\n    logger.warning(\n        \"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\",\n        args.local_rank,\n        device,\n        args.n_gpu,\n        bool(args.local_rank != -1),\n        args.fp16,\n    )\n\n    # Set seed\n    set_seed(args)\n\n    # Load model and tokenizer with updated class\n    config = AutoConfig.from_pretrained(args.config_name, cache_dir=args.cache_dir)\n    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, cache_dir=args.cache_dir)\n    model = AutoModelForCausalLM.from_pretrained(  # Updated class for language model compatibility\n        args.model_name_or_path,\n        from_tf=False,\n        config=config,\n        cache_dir=args.cache_dir,\n    )\n    model.to(args.device)\n\n    logger.info(\"Training/evaluation parameters %s\", args)\n\n    # Training\n    if args.do_train:\n        train_dataset = load_and_cache_examples(args, tokenizer, df_trn, df_val, evaluate=False)\n\n        global_step, tr_loss = train(args, train_dataset, model, tokenizer)\n        logger.info(\" global_step = %s, average loss = %s\", global_step, tr_loss)\n\n    # Save the model and tokenizer\n    if args.do_train:\n        os.makedirs(args.output_dir, exist_ok=True)\n        logger.info(\"Saving model checkpoint to %s\", args.output_dir)\n        model.save_pretrained(args.output_dir)\n        tokenizer.save_pretrained(args.output_dir)\n        torch.save(args, os.path.join(args.output_dir, \"training_args.bin\"))\n\n        # Reload the trained model for evaluation\n        model = AutoModelForCausalLM.from_pretrained(args.output_dir)\n        tokenizer = AutoTokenizer.from_pretrained(args.output_dir)\n        model.to(args.device)\n\n    # Evaluation\n    results = {}\n    if args.do_eval and args.local_rank in [-1, 0]:\n        checkpoints = [args.output_dir]\n        if args.eval_all_checkpoints:\n            checkpoints = list(\n                os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + WEIGHTS_NAME, recursive=True))\n            )\n            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n        for checkpoint in checkpoints:\n            global_step = checkpoint.split(\"-\")[-1] if len(checkpoints) > 1 else \"\"\n            prefix = checkpoint.split(\"/\")[-1] if checkpoint.find(\"checkpoint\") != -1 else \"\"\n\n            model = AutoModelForCausalLM.from_pretrained(checkpoint)  # Updated class\n            model.to(args.device)\n            result = evaluate(args, model, tokenizer, df_trn, df_val, prefix=prefix)\n            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n            results.update(result)\n\n    return results\n","metadata":{"id":"-MGD6bFXV4Z-","executionInfo":{"status":"ok","timestamp":1746435390239,"user_tz":-60,"elapsed":15,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T13:23:39.940156Z","iopub.execute_input":"2025-05-06T13:23:39.940453Z","iopub.status.idle":"2025-05-06T13:23:39.960819Z","shell.execute_reply.started":"2025-05-06T13:23:39.940430Z","shell.execute_reply":"2025-05-06T13:23:39.960150Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"It is time to train our model!\n\n![alt text](https://media4.giphy.com/media/v1.Y2lkPTc5MGI3NjExbm44c3I5OWJxaGgzZTkwYzZ2cDJ4N2FuZnpkcTI1YW4zeDJ6YW44dCZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3ohs83cvmud7ThYTzq/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)","metadata":{"id":"UZEHDzR0Vjs7"}},{"cell_type":"code","source":"main(trn_df, val_df)","metadata":{"id":"__iqR8YFV-Ex","outputId":"94607196-7bb2-4627-dd4e-e8d79aec9338","executionInfo":{"status":"error","timestamp":1746444749779,"user_tz":-60,"elapsed":9346242,"user":{"displayName":"Ghada Ataoui","userId":"17490217075797873512"}},"trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:57.206352Z","iopub.execute_input":"2025-05-06T15:20:57.206574Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Epoch:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"107778f6cf94458ca725c6ca46eaee74"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Iteration:   0%|          | 0/6119 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4b01bfca7bc14b0aa1c7fa579febba72"}},"metadata":{}}],"execution_count":null},{"cell_type":"markdown","source":"## Chatting with Sheldon","metadata":{"id":"6eDkPEuvbD47"}},{"cell_type":"markdown","source":"The model is ready, so it's time to chat with Sheldon. But don't forget that Sheldon can be rude, I warned you.\n\nA variety of methods can be used in responces generation. You can find more details about these methods by this [link](https://huggingface.co/blog/how-to-generate).\n\n![alt text](https://media1.giphy.com/media/v1.Y2lkPTc5MGI3NjExMW9tbGp5dXZsMGYxMHB4dHQwdncyMmJjczRuZGJwNjY2amNrbHBmbiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/l2R01AMRh4g9XEKZO/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)","metadata":{"id":"rjVqotI05gOS"}},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelWithLMHead\nimport torch\n\ntokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-small')\nmodel = AutoModelWithLMHead.from_pretrained('output-small')\n\n# Predefined list of user messages\nuser_inputs = [\n    \"Hey, who are you?\",\n    \"What can you do?\",\n    \"Tell me a joke.\",\n    \"Do you know Rick and Morty?\",\n    \"Bye!\"\n]\n\nchat_history_ids = None\n\nfor step in range(5):\n    new_user_input_ids = tokenizer.encode(user_inputs[step] + tokenizer.eos_token, return_tensors='pt')\n\n    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n\n    chat_history_ids = model.generate(\n        bot_input_ids, max_length=200,\n        pad_token_id=tokenizer.eos_token_id,\n        no_repeat_ngram_size=3,\n        do_sample=True,\n        top_k=100,\n        top_p=0.7,\n        temperature=0.8\n    )\n\n    response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)\n    print(f\"SheldonBot: {response}\")\n","metadata":{"id":"nIeqMwZktv7N","trusted":true,"execution":{"iopub.status.busy":"2025-05-06T15:20:53.229475Z","iopub.execute_input":"2025-05-06T15:20:53.229817Z","iopub.status.idle":"2025-05-06T15:20:57.204051Z","shell.execute_reply.started":"2025-05-06T15:20:53.229791Z","shell.execute_reply":"2025-05-06T15:20:57.203274Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/models/auto/modeling_auto.py:1945: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"SheldonBot:  I’m Zack.\nSheldonBot:  What can I do? I‘m a famous actress, I win awards, I get a MacArthur Grant, I have a big role in a movie.\nSheldonBot:  I can’t tell you a joke, I”m Jewish.\nSheldonBot: !!!!.!!\nSheldonBot: !!.Bye, buddy!! Hey!! You guys got any more Sailor Moon action figures?\n","output_type":"stream"}],"execution_count":23},{"cell_type":"markdown","source":"![alt text](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExMjBicDAwaGgybTYwZW9oajgweXB2Z3Z3MDNiYmNhMmJrcG11dXljNyZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3osxY6y28AlWROLzqw/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)\n","metadata":{"id":"K1gX23iKkd0g"}},{"cell_type":"markdown","source":"## Conclusion","metadata":{"id":"Woi683Eyvttc"}},{"cell_type":"markdown","source":"![alt text](https://media2.giphy.com/media/v1.Y2lkPTc5MGI3NjExaWZiaGY2OWo1MTRyb2FjMXM5b2JucGlyMGY1cmR6a29kdzNldGV1eSZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/3ohs4nAq7JaOydmxO0/giphy.gif)\n\nImage from [Giphy](https://giphy.com/)\n\nCongratulations! Our virtual Sheldon is alive (almost)! With the help of fine-tuning our model on a small dataset, we were able to create a virtual character with whom we can conduct interesting dialogs.\n\nUsing the proposed approach you can create many interesting virtual characters based on an arbitrary dialogs dataset (just a csv file with replicas, one replica per line).","metadata":{"id":"aZmkXDkv4BAl"}}]}